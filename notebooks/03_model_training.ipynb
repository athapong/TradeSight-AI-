{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training for Trading Entry Points\n",
    "\n",
    "This notebook demonstrates the process of training and evaluating our trading entry point prediction model. We'll cover:\n",
    "- Loading the prepared dataset\n",
    "- Train/test splitting\n",
    "- Model training\n",
    "- Evaluation and validation\n",
    "- Model explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to import from src\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "\n",
    "# Import from src modules\n",
    "from src.data.loader import load_data, preprocess_data\n",
    "from src.data.features import prepare_features\n",
    "from src.models.random_forest_model import RandomForestModel\n",
    "from src.visualization.charts import plot_feature_importance\n",
    "from src.utils.helpers import set_pandas_display_options\n",
    "\n",
    "# Set display options\n",
    "set_pandas_display_options()\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Prepared Dataset\n",
    "\n",
    "Let's load the dataset we prepared in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load processed data from file\n",
    "try:\n",
    "    df_features = pd.read_csv('../processed_data.csv', index_col=0, parse_dates=True)\n",
    "    print(f\"Loaded processed dataset with {df_features.shape[1]} columns and {df_features.shape[0]} rows\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Processed data file not found. Preparing features from raw data...\")\n",
    "    # Load and process raw data\n",
    "    file_path = '../USATECH.IDXUSD_Candlestick_15_M_BID_01.01.2023-18.01.2025.csv'\n",
    "    df_raw = load_data(file_path)\n",
    "    df = preprocess_data(df_raw)\n",
    "    \n",
    "    # Set parameters\n",
    "    future_periods = 10\n",
    "    profit_target = 0.01\n",
    "    stop_loss = 0.005\n",
    "    \n",
    "    # Prepare features\n",
    "    df_features = prepare_features(\n",
    "        df, \n",
    "        include_target=True, \n",
    "        future_periods=future_periods, \n",
    "        profit_target=profit_target, \n",
    "        stop_loss=stop_loss\n",
    "    )\n",
    "    print(f\"Prepared dataset with {df_features.shape[1]} columns and {df_features.shape[0]} rows\")\n",
    "\n",
    "# Display a few rows\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split\n",
    "\n",
    "For time series data, we need to be careful about how we split the data. We'll use a time-based split rather than a random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "test_size = 0.2  # Last 20% for testing\n",
    "\n",
    "# Calculate split index\n",
    "split_idx = int(len(df_features) * (1 - test_size))\n",
    "df_train = df_features.iloc[:split_idx]\n",
    "df_test = df_features.iloc[split_idx:]\n",
    "\n",
    "print(f\"Training set: {df_train.shape} from {df_train.index.min()} to {df_train.index.max()}\")\n",
    "print(f\"Testing set: {df_test.shape} from {df_test.index.min()} to {df_test.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Target Distribution in Train/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distribution in training set\n",
    "train_target_counts = df_train['Target'].value_counts()\n",
    "test_target_counts = df_test['Target'].value_counts()\n",
    "\n",
    "print(\"Training Set Target Distribution:\")\n",
    "for target, count in train_target_counts.items():\n",
    "    print(f\"  Target {target}: {count} samples ({count/len(df_train)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nTest Set Target Distribution:\")\n",
    "for target, count in test_target_counts.items():\n",
    "    print(f\"  Target {target}: {count} samples ({count/len(df_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Training set\n",
    "axes[0].bar(['Short (-1)', 'Neutral (0)', 'Long (1)'], \n",
    "          [train_target_counts.get(-1, 0), train_target_counts.get(0, 0), train_target_counts.get(1, 0)],\n",
    "          color=['red', 'gray', 'green'])\n",
    "axes[0].set_title('Training Set Target Distribution')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set\n",
    "axes[1].bar(['Short (-1)', 'Neutral (0)', 'Long (1)'], \n",
    "          [test_target_counts.get(-1, 0), test_target_counts.get(0, 0), test_target_counts.get(1, 0)],\n",
    "          color=['red', 'gray', 'green'])\n",
    "axes[1].set_title('Test Set Target Distribution')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and Train Model\n",
    "\n",
    "Now let's create and train our Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = RandomForestModel(\n",
    "    n_estimators=100,\n",
    "    max_depth=12,\n",
    "    min_samples_split=10,\n",
    "    random_state=42,\n",
    "    profit_target=0.01,\n",
    "    stop_loss=0.005\n",
    ")\n",
    "\n",
    "# Extract features and target\n",
    "X_train, y_train = model.extract_features_target(df_train)\n",
    "X_test, y_test = model.extract_features_target(df_test)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "%time model.train(X_train, y_train)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "Let's evaluate the model's performance on both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training data\n",
    "train_metrics = model.evaluate(X_train, y_train)\n",
    "print(\"Training set metrics:\")\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_metrics = model.evaluate(X_test, y_test)\n",
    "print(\"\\nTest set metrics:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed classification report\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=['Short', 'Neutral', 'Long'] if len(set(y_test)) == 3 else ['Neutral', 'Long'],\n",
    "           yticklabels=['Short', 'Neutral', 'Long'] if len(set(y_test)) == 3 else ['Neutral', 'Long'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis\n",
    "\n",
    "Let's analyze which features were most important in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = model.get_feature_importance()\n",
    "print(\"Top 15 important features:\")\n",
    "feature_importance.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "fig = plot_feature_importance(feature_importance, top_n=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation with Time Series Split\n",
    "\n",
    "Let's perform time series cross-validation to get a more robust estimate of our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all features and target\n",
    "X, y = model.extract_features_target(df_features)\n",
    "\n",
    "# Set up time series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Performance metrics for each fold\n",
    "cv_scores = []\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    # Get train/test split for this fold\n",
    "    X_train_fold, X_test_fold = X[train_idx], X[test_idx]\n",
    "    y_train_fold, y_test_fold = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Create and train a new model\n",
    "    fold_model = RandomForestModel(\n",
    "        n_estimators=100,\n",
    "        max_depth=12,\n",
    "        min_samples_split=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    fold_model.train(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics = fold_model.evaluate(X_test_fold, y_test_fold)\n",
    "    cv_scores.append(metrics)\n",
    "    \n",
    "    print(f\"Fold {len(cv_scores)} - Accuracy: {metrics['accuracy']:.4f}, F1 Score: {metrics['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average metrics across folds\n",
    "avg_metrics = {}\n",
    "for metric in cv_scores[0].keys():\n",
    "    avg_metrics[metric] = np.mean([fold[metric] for fold in cv_scores])\n",
    "\n",
    "print(\"Average metrics across folds:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction Explanation\n",
    "\n",
    "Let's examine some specific predictions and get explanations for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test set predictions\n",
    "test_pred = model.predict(X_test)\n",
    "test_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Find some interesting predictions\n",
    "# 1. A high confidence correct prediction\n",
    "high_conf_correct = []\n",
    "# 2. A low confidence prediction\n",
    "low_conf = []\n",
    "# 3. An incorrect prediction\n",
    "incorrect = []\n",
    "\n",
    "for i in range(len(test_pred)):\n",
    "    pred = test_pred[i]\n",
    "    actual = y_test[i]\n",
    "    \n",
    "    # Get prediction probability\n",
    "    if pred == 0:\n",
    "        prob = test_proba[i][0]\n",
    "    elif pred == 1 and len(test_proba[i]) > 1:\n",
    "        prob = test_proba[i][1]\n",
    "    elif pred == -1 and len(test_proba[i]) > 2:\n",
    "        prob = test_proba[i][2]\n",
    "    else:\n",
    "        prob = test_proba[i][0]\n",
    "    \n",
    "    # Check prediction categories\n",
    "    if pred == actual and prob > 0.8 and len(high_conf_correct) < 1:\n",
    "        high_conf_correct.append(i)\n",
    "    elif 0.5 < prob < 0.6 and len(low_conf) < 1:\n",
    "        low_conf.append(i)\n",
    "    elif pred != actual and len(incorrect) < 1:\n",
    "        incorrect.append(i)\n",
    "    \n",
    "    # Break if we found examples for all categories\n",
    "    if len(high_conf_correct) >= 1 and len(low_conf) >= 1 and len(incorrect) >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get explanations for interesting predictions\n",
    "if high_conf_correct:\n",
    "    print(\"Explanation for high confidence correct prediction:\")\n",
    "    idx = high_conf_correct[0]\n",
    "    explanation = model.explain_prediction(X_test[idx], test_pred[idx], test_proba[idx])\n",
    "    print(f\"Date: {df_test.index[idx]}\")\n",
    "    print(f\"Actual: {y_test[idx]}, Predicted: {test_pred[idx]}, Confidence: {explanation['confidence']:.2%}\")\n",
    "    print(explanation['explanation'])\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "if low_conf:\n",
    "    print(\"\\nExplanation for low confidence prediction:\")\n",
    "    idx = low_conf[0]\n",
    "    explanation = model.explain_prediction(X_test[idx], test_pred[idx], test_proba[idx])\n",
    "    print(f\"Date: {df_test.index[idx]}\")\n",
    "    print(f\"Actual: {y_test[idx]}, Predicted: {test_pred[idx]}, Confidence: {explanation['confidence']:.2%}\")\n",
    "    print(explanation['explanation'])\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "if incorrect:\n",
    "    print(\"\\nExplanation for incorrect prediction:\")\n",
    "    idx = incorrect[0]\n",
    "    explanation = model.explain_prediction(X_test[idx], test_pred[idx], test_proba[idx])\n",
    "    print(f\"Date: {df_test.index[idx]}\")\n",
    "    print(f\"Actual: {y_test[idx]}, Predicted: {test_pred[idx]}, Confidence: {explanation['confidence']:.2%}\")\n",
    "    print(explanation['explanation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Trained Model\n",
    "\n",
    "Let's save our trained model for use in backtesting and future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = model.save('../trained_model.pkl')\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've accomplished the following:\n",
    "\n",
    "1. Loaded our preprocessed dataset\n",
    "2. Split the data into training and test sets using a time-based approach\n",
    "3. Created and trained a Random Forest model for predicting trading entry points\n",
    "4. Evaluated the model's performance on both training and test data\n",
    "5. Analyzed feature importance to understand what drives the model's predictions\n",
    "6. Performed time series cross-validation for more robust performance estimates\n",
    "7. Generated explanations for specific predictions\n",
    "8. Saved our trained model for future use\n",
    "\n",
    "Next, we'll use our trained model for backtesting to see how it would have performed in historical trading scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
