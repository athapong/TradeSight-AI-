{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Trading\n",
    "\n",
    "This notebook demonstrates the feature engineering process for our trading model, including:\n",
    "- Adding technical indicators\n",
    "- Creating price and volume features\n",
    "- Generating target variables\n",
    "- Analyzing feature correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to import from src\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import from src modules\n",
    "from src.data.loader import load_data, preprocess_data\n",
    "from src.data.features import (add_technical_indicators, add_price_features, \n",
    "                              add_volume_features, generate_target,\n",
    "                              prepare_features)\n",
    "from src.utils.helpers import set_pandas_display_options\n",
    "\n",
    "# Set display options\n",
    "set_pandas_display_options()\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 15-minute data\n",
    "file_path = '../USATECH.IDXUSD_Candlestick_15_M_BID_01.01.2023-18.01.2025.csv'\n",
    "df_raw = load_data(file_path)\n",
    "df = preprocess_data(df_raw)\n",
    "\n",
    "print(f\"Loaded {len(df)} rows from {df.index.min()} to {df.index.max()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add Technical Indicators\n",
    "\n",
    "Let's add common technical indicators to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add technical indicators\n",
    "df_indicators = add_technical_indicators(df)\n",
    "\n",
    "# Display the new columns\n",
    "new_columns = [col for col in df_indicators.columns if col not in df.columns]\n",
    "print(f\"Added {len(new_columns)} new technical indicators:\")\n",
    "print(new_columns)\n",
    "\n",
    "# Show first few rows with indicators\n",
    "df_indicators[new_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Technical Indicators\n",
    "\n",
    "Let's visualize some of the technical indicators to understand their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a recent time period for visualization\n",
    "recent_data = df_indicators.iloc[-200:]\n",
    "\n",
    "# Create a 2x2 grid of subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Price and Moving Averages\n",
    "axes[0, 0].plot(recent_data.index, recent_data['Close'], label='Close')\n",
    "axes[0, 0].plot(recent_data.index, recent_data['SMA20'], label='SMA20')\n",
    "axes[0, 0].plot(recent_data.index, recent_data['SMA50'], label='SMA50')\n",
    "axes[0, 0].plot(recent_data.index, recent_data['SMA200'], label='SMA200')\n",
    "axes[0, 0].set_title('Price and Moving Averages')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Price')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Plot 2: Bollinger Bands\n",
    "axes[0, 1].plot(recent_data.index, recent_data['Close'], label='Close')\n",
    "axes[0, 1].plot(recent_data.index, recent_data['BB_Upper'], label='Upper BB')\n",
    "axes[0, 1].plot(recent_data.index, recent_data['BB_Middle'], label='Middle BB')\n",
    "axes[0, 1].plot(recent_data.index, recent_data['BB_Lower'], label='Lower BB')\n",
    "axes[0, 1].fill_between(recent_data.index, \n",
    "                       recent_data['BB_Upper'], \n",
    "                       recent_data['BB_Lower'], \n",
    "                       alpha=0.2, color='gray')\n",
    "axes[0, 1].set_title('Bollinger Bands')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Price')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Plot 3: RSI\n",
    "axes[1, 0].plot(recent_data.index, recent_data['RSI'], label='RSI')\n",
    "axes[1, 0].axhline(y=70, color='r', linestyle='--', alpha=0.5, label='Overbought (70)')\n",
    "axes[1, 0].axhline(y=30, color='g', linestyle='--', alpha=0.5, label='Oversold (30)')\n",
    "axes[1, 0].set_title('Relative Strength Index (RSI)')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('RSI')\n",
    "axes[1, 0].set_ylim(0, 100)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Plot 4: MACD\n",
    "axes[1, 1].plot(recent_data.index, recent_data['MACD'], label='MACD')\n",
    "axes[1, 1].plot(recent_data.index, recent_data['MACD_Signal'], label='Signal Line')\n",
    "axes[1, 1].bar(recent_data.index, recent_data['MACD_Hist'], \n",
    "              color=['green' if x > 0 else 'red' for x in recent_data['MACD_Hist']], \n",
    "              alpha=0.5, label='Histogram')\n",
    "axes[1, 1].set_title('MACD')\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Price and Volume Features\n",
    "\n",
    "Now let's add additional price-based and volume-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add price features\n",
    "df_price = add_price_features(df_indicators)\n",
    "\n",
    "# Display the new columns\n",
    "new_price_columns = [col for col in df_price.columns if col not in df_indicators.columns]\n",
    "print(f\"Added {len(new_price_columns)} new price features:\")\n",
    "print(new_price_columns)\n",
    "\n",
    "# Show first few rows with price features\n",
    "df_price[new_price_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add volume features\n",
    "df_volume = add_volume_features(df_price)\n",
    "\n",
    "# Display the new columns\n",
    "new_volume_columns = [col for col in df_volume.columns if col not in df_price.columns]\n",
    "print(f\"Added {len(new_volume_columns)} new volume features:\")\n",
    "print(new_volume_columns)\n",
    "\n",
    "# Show first few rows with volume features\n",
    "df_volume[new_volume_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Target Variable\n",
    "\n",
    "Now let's create a target variable based on future price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate target variable\n",
    "future_periods = 10  # Look ahead 10 periods\n",
    "profit_target = 0.01  # 1% profit target\n",
    "stop_loss = 0.005  # 0.5% stop loss\n",
    "\n",
    "df_target = generate_target(df_volume, future_periods, profit_target, stop_loss)\n",
    "\n",
    "# Check target distribution\n",
    "target_counts = df_target['Target'].value_counts()\n",
    "print(\"Target Distribution:\")\n",
    "print(target_counts)\n",
    "print(f\"Percentage of long signals: {target_counts[1] / len(df_target) * 100:.2f}%\")\n",
    "print(f\"Percentage of short signals: {target_counts[-1] / len(df_target) * 100:.2f}%\")\n",
    "print(f\"Percentage of neutral: {target_counts[0] / len(df_target) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(['Short (-1)', 'Neutral (0)', 'Long (1)'], \n",
    "              [target_counts.get(-1, 0), target_counts.get(0, 0), target_counts.get(1, 0)],\n",
    "              color=['red', 'gray', 'green'])\n",
    "\n",
    "# Add percentage labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height/len(df_target)*100:.1f}%',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.title('Target Variable Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Correlation Analysis\n",
    "\n",
    "Let's analyze which features are most correlated with our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation with target\n",
    "target_corr = df_target.corr()['Target'].sort_values(ascending=False)\n",
    "\n",
    "# Print top positive correlations\n",
    "print(\"Top Positive Correlations with Target:\")\n",
    "print(target_corr.head(15))\n",
    "\n",
    "# Print top negative correlations\n",
    "print(\"\\nTop Negative Correlations with Target:\")\n",
    "print(target_corr.tail(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = pd.concat([target_corr.head(10), target_corr.tail(10)])\n",
    "top_features = top_features[top_features.index != 'Target']  # Remove target itself\n",
    "colors = ['green' if c > 0 else 'red' for c in top_features]\n",
    "plt.barh(top_features.index, top_features, color=colors)\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.title('Top Correlated Features with Target')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Preparation Pipeline\n",
    "\n",
    "Now let's use our complete feature preparation pipeline to get a clean dataset ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all features in one go\n",
    "df_features = prepare_features(\n",
    "    df, \n",
    "    include_target=True, \n",
    "    future_periods=future_periods, \n",
    "    profit_target=profit_target, \n",
    "    stop_loss=stop_loss\n",
    ")\n",
    "\n",
    "print(f\"Final dataset shape: {df_features.shape}\")\n",
    "print(f\"Number of features: {df_features.shape[1] - 1}\")  # Excluding target\n",
    "print(f\"Number of samples: {df_features.shape[0]}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Selection Analysis\n",
    "\n",
    "Let's perform basic feature selection to understand which features might be most predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Define features and target\n",
    "X = df_features.drop(['Target'], axis=1)\n",
    "y = df_features['Target']\n",
    "\n",
    "# Replace any remaining NaN or infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Train a Random Forest for feature importance\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print top 20 features\n",
    "print(\"Top 20 features by importance:\")\n",
    "for i in range(20):\n",
    "    print(f\"{i+1}. {X.columns[indices[i]]}: {importances[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Feature Importances from Random Forest\")\n",
    "plt.bar(range(20), importances[indices[:20]], align=\"center\")\n",
    "plt.xticks(range(20), X.columns[indices[:20]], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Processed Dataset\n",
    "\n",
    "Finally, let's save our processed dataset for use in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "df_features.to_csv('../processed_data.csv')\n",
    "print(f\"Saved processed dataset with {df_features.shape[1]} columns and {df_features.shape[0]} rows to '../processed_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've completed the following steps:\n",
    "\n",
    "1. Loaded and preprocessed the USATECH index data\n",
    "2. Added technical indicators like moving averages, RSI, MACD, and Bollinger Bands\n",
    "3. Created price and volume-based features\n",
    "4. Generated a target variable based on future price movements\n",
    "5. Analyzed feature correlations with the target\n",
    "6. Used our complete feature preparation pipeline\n",
    "7. Performed feature importance analysis\n",
    "8. Saved the processed dataset for use in model training\n",
    "\n",
    "Next, we'll use this processed dataset to train our trading model in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
